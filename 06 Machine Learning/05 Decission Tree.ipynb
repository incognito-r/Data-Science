{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decission Tree\n",
    "A Decision Tree is a versatile supervised machine learning algorithm used for both classification and regression tasks. It is based on a tree-like structure where internal nodes represent decision points based on features, branches represent outcomes, and leaf nodes represent final predictions (class labels or values).\n",
    "\n",
    "- Type of Algorithm: Supervised Learning (Classification & Regression).\n",
    "- Structure:\n",
    "    - Root Node: The first decision point.\n",
    "    - Internal Nodes: Intermediate decision points.\n",
    "    - Leaf Nodes: Final outcomes (class labels or values).\n",
    "- Splitting Criteria:\n",
    "    - Classification: Gini Impurity, Entropy (Information Gain).\n",
    "    - Regression: Variance Reduction, Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it Works**:\n",
    "\n",
    "- Starts from the root node and splits the dataset based on the feature that best separates the target variable.\n",
    "- Continues splitting until a stopping criterion is met, such as:\n",
    "    - Maximum depth of the tree.\n",
    "    - Minimum number of samples in a node.\n",
    "    - No significant gain from further splitting.\n",
    "- The result is a tree structure where each leaf node represents a class (for classification) or a value (for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications**:\n",
    "\n",
    "- Healthcare: Disease diagnosis based on symptoms.\n",
    "- Finance: Credit scoring, fraud detection.\n",
    "- Marketing: Customer segmentation, churn prediction.\n",
    "- Education: Predicting student performance.\n",
    "- Retail: Product recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Advantages**                                                                 | **Disadvantages**                                                                  |\n",
    "|--------------------------------------------------------------------------------|------------------------------------------------------------------------------------|\n",
    "| Simple to understand, interpret, and visualize.                                | Prone to overfitting, especially with deep trees.                                 |\n",
    "| No assumptions about the data distribution (non-parametric).                   | Sensitive to small changes in data, leading to different tree structures.         |\n",
    "| Can handle both categorical and numerical data.                                | Biased towards features with more levels (for categorical variables).             |\n",
    "| Captures non-linear relationships naturally.                                   | Not robust to noisy data and irrelevant features.                                 |\n",
    "| Works well for small to medium-sized datasets.                                 | Struggles with high-dimensional data and sparse datasets.                         |\n",
    "| Can handle missing values (by using surrogate splits or ignoring them).        | Less accurate compared to ensemble methods like Random Forest or Gradient Boosting.|\n",
    "| Computationally efficient and fast to train.                                   | Requires careful pruning or hyperparameter tuning to prevent overfitting.         |\n",
    "| Effective for multi-class classification problems.                             | Struggles with imbalanced datasets (needs resampling or weighted splitting).      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
