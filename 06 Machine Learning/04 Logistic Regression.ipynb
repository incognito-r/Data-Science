{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic Regression is a supervised machine learning algorithm used for classification tasks. It predicts the probability of a dependent variable belonging to a particular category, typically for binary or multi-class classification problems. Despite its name, Logistic Regression is used for classification, not regression.\n",
    "\n",
    "- Type of Algorithm: Supervised Learning (Classification)\n",
    "- Target Variable: Categorical (e.g., 0/1, Yes/No, Spam/Not Spam)\n",
    "- Output: Probability values, which are then converted into class labels using a threshold (e.g., 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Representation:\n",
    "\n",
    "**Sigmoid Function**:\n",
    "The sigmoid function is used to model the probability of the dependent variable being 1, expressed as:\n",
    "\n",
    "$$ P(Y=1|X) = \\frac{1}{1 + e^{-z}}, $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ z = b₀ + b₁X₁ + b₂X₂ + \\dots + bₙXₙ $ \n",
    "- P(Y=1|X): Probability of the dependent variable being 1.\n",
    "- b₀, b₁, ..., bₙ: Coefficients of the model.\n",
    "- X₁, X₂, ..., Xₙ: Features of the data.\n",
    "\n",
    "The classification decision is based on a threshold, typically set to 0.5:\n",
    "\n",
    "- **If** \\( P(Y=1|X) > 0.5 \\), predict class **1**.\n",
    "- **If** \\( P(Y=1|X) < 0.5 \\), predict class **0**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Logistic Regression:\n",
    "- Binary Logistic Regression: Two possible outcomes (e.g., Spam/Not Spam).\n",
    "- Multinomial Logistic Regression: More than two classes, not ordered (e.g., Types of fruits).\n",
    "- Ordinal Logistic Regression: More than two classes, with an inherent order (e.g., Customer satisfaction ratings: Poor, Average, Good)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications**:\n",
    "- Healthcare: Predicting the likelihood of diseases (e.g., diabetes prediction).\n",
    "- Marketing: Predicting customer churn or purchase likelihood.\n",
    "- Finance: Credit risk assessment, fraud detection.\n",
    "- Natural Language Processing: Spam email detection, sentiment analysis.\n",
    "- E-commerce: Predicting whether a user will click on an ad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Advantages**                                                              | **Disadvantages**                                                                 |\n",
    "|------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| Simple to implement and easy to interpret.                                   | Assumes a linear relationship between features and the log-odds of the target.    |\n",
    "| Outputs probabilities, which can be useful for ranking and decision-making.  | Not effective for non-linear relationships unless features are transformed.       |\n",
    "| Computationally efficient and works well with small to medium-sized datasets.| Sensitive to outliers, which can distort predictions.                             |\n",
    "| Works well for linearly separable data.                                      | Assumes no multicollinearity among features (independence of predictors).         |\n",
    "| Can handle binary and multi-class classification problems.                   | Performs poorly on highly imbalanced datasets without proper preprocessing.       |\n",
    "| Can be regularized (L1, L2) to prevent overfitting.                          | Limited to classification tasks, not suitable for regression or complex problems. |\n",
    "| Easy to update the model with new data (incremental learning).               | Struggles with large feature spaces without proper dimensionality reduction.      |\n",
    "| Probabilistic predictions allow for uncertainty measurement.                 | Assumes features are independent, which may not hold in real-world data.          |\n",
    "| Well-suited for baseline models in classification tasks.                     | Requires proper feature scaling for optimal performance.                          |\n",
    "| The mathematical foundation is simple and widely understood.                 | Not robust to noisy or irrelevant features.                                       |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
