{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "- Best \"default\" optimizer for most deep learning tasks.\n",
    "- Good for complex models (CNNs, RNNs, Transformers).\n",
    "- Adapts learning rate dynamically for each parameter.\n",
    "\n",
    "### SGD\n",
    "- Great for traditional tasks like vision (ImageNet training).\n",
    "- Use with momentum to speed up convergence and avoid local minima.\n",
    "- Slower than Adam but often better for fine-tuning or stable training.\n",
    "\n",
    "### RMSProp:\n",
    "- Best for RNNs or tasks with noisy gradients (e.g., time series).\n",
    "- Works well for non-convex problems.\n",
    "\n",
    "### Adagrad/Adadelta:\n",
    "- Use for sparse datasets (e.g., NLP tasks like word embeddings).\n",
    "- Adadelta solves Adagrad's problem of shrinking learning rates.\n",
    "\n",
    "### AdamW:\n",
    "- Use when regularization is important (e.g., Transformers like BERT or GPT).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
